{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/web-text-corpus/webtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Missing Value on Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_null_df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "dictionary_domain = {}\n",
    "for domain in no_null_df['domain']:\n",
    "    cnt[domain] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We take 30 samples/sentences which including 15 data on Firefox class, 15 data on Overheard class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_firefox = no_null_df[no_null_df['domain'] == 'firefox'].iloc[:15,:]\n",
    "df_sample_overheard = no_null_df[no_null_df['domain'] == 'overheard'].iloc[:15,:]\n",
    "df_sample = pd.concat([df_sample_firefox, df_sample_overheard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_df = df_sample.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_df['text'] = prepro_df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuations, HTML, URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(document):\n",
    "    #Define the Regex\n",
    "    regex_html = '</?.*/?>'\n",
    "    regex_url = '(https?://)|(https?:\\/\\/)?(www\\.)?([-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*))'\n",
    "    regex_punc = '[!\\(\\)-\\[\\]{};:\"\\'\\,<>\\./\\?@#$%^&*_~]'\n",
    "    regex_space = '\\s{2,}'\n",
    "    \n",
    "    #Replace if match with Regex pattern\n",
    "    document['text'] = document['text'].str.replace(regex_html, ' ')\n",
    "    document['text'] = document['text'].str.replace(regex_url, ' ')\n",
    "    document['text'] = document['text'].str.replace(regex_punc, '')\n",
    "    document['text'] = document['text'].str.replace(regex_space, ' ')\n",
    "    \n",
    "    #Reset index\n",
    "    document = document.reset_index(drop=True)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansing_df = cleansing(prepro_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleansing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = cleansing_df[['text']]\n",
    "df_y = cleansing_df[['domain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(df_x, df_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords\n",
    "Some references I found according to Stopwords Analysis:\n",
    "1. https://www.sciencedirect.com/science/article/pii/S1877050914013799\n",
    "2. https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html\n",
    "3. https://medium.com/@jasoncrease/zipf-54912d5651cc\n",
    "4. https://medium.com/@devalshah1619/a-mysterious-law-so-simple-and-yet-so-universal-aa9f1c8903d1\n",
    "5. https://www.slideshare.net/Staano/stopwords\n",
    "6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Zipf Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "print('\\n{} terms'.format(len(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rows indicate the documents, while column indicate the term.\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count total occurences of each Term in Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_idf(document):\n",
    "    #init variable\n",
    "    term_idf = []\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(document)\n",
    "    term = vectorizer.get_feature_names()\n",
    "    term_array = X.toarray()\n",
    "    \n",
    "    #count IDF (Inverse Document Frequency) for each Term\n",
    "    for i in range(len(term)):\n",
    "        df = sum([1 for tf in term_array[:, i] if tf >= 1])\n",
    "        idf = np.log10(len(document) / df)\n",
    "        term_idf.append((term[i], idf))\n",
    "    return term_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurences(document):\n",
    "    #init variable\n",
    "    term_occurences = []\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(document)\n",
    "    term = vectorizer.get_feature_names()\n",
    "    term_array = X.toarray()\n",
    "    \n",
    "    #count total term on each doc\n",
    "    for i in range(len(term)):\n",
    "        count_term = sum(term_array[:, i])\n",
    "        term_occurences.append((term[i], count_term))\n",
    "    return term_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences = count_occurences(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_each_term = count_idf(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_each_term.sort(reverse=True, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_each_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences.sort(reverse=True, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_occurences = [ item[0] for item in occurences]\n",
    "value_occurences = [ item[1] for item in occurences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zipf(terms: [], freq: []):\n",
    "    fig = plt.figure(figsize=(40,20))\n",
    "    plt.plot(terms, freq)\n",
    "    plt.xticks(terms, rotation='vertical')\n",
    "    plt.xlabel('Terms')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zipf(key_occurences, value_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_tuple(list_of_tuple, columns):\n",
    "    list_of_tuple.sort(reverse=True, key=lambda x: x[1])\n",
    "    df_ = pd.DataFrame(list_of_tuple, columns=columns)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "occurences_df = create_dataframe_tuple(occurences, columns=['terms', 'freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idf_df = create_dataframe_tuple(list_of_tuple=idf_each_term, columns=['terms', 'idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Paper....\n",
    "There are several methods on removing Stopwords, in Zipf's Law (Z-Methods):\n",
    "Three stopword creation methods are used in addition to the classic stoplist. This includes removing most frequent words (TF-High), removing words that occur once, i.e., singleton words (TF1), and removing words with low inverse document frequency (IDF) (Jashanjot, Buttar, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_freq_zipf(dataframe_tf, dataframe_idf, threshold_idf):\n",
    "    most_freq = max(dataframe_tf['freq'])\n",
    "    #singleton_word = 1\n",
    "    filter_tf = dataframe_tf[(dataframe_tf['freq'] == most_freq)] #| (dataframe_tf['freq'] == singleton_word)]\n",
    "    filter_idf = dataframe_idf[(dataframe_idf['idf'] <= threshold_idf)]\n",
    "    print(filter_tf)\n",
    "    print(filter_idf)\n",
    "    stopwords = np.concatenate((filter_tf['terms'], filter_idf['terms']))\n",
    "    return list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopword_zipf = filter_freq_zipf(occurences_df, idf_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_zipf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info_feature(document):\n",
    "    mi = []\n",
    "    vect = CountVectorizer()\n",
    "    X = vect.fit_transform(document[0]['text'])\n",
    "    terms = vect.get_feature_names()\n",
    "    mutual_term = mutual_info_classif(X=X, y=document[1]['domain'])\n",
    "    for term, mutual in zip(terms, mutual_term):\n",
    "        mi.append((term, mutual))\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_information = mutual_info_feature([X_train, y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_df = pd.DataFrame(mutual_information, columns=['terms', 'mutual information'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(mi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because minimum MI is relative value, we used rank to get the terms with miniumum MI\n",
    "rank = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the most minimum Mutual Information on a Sorted DF\n",
    "def filter_minimum_mi(tuple_of_list, rank):\n",
    "    tuple_of_list.sort(reverse=False, key=lambda x: x[1])\n",
    "    df_ = pd.DataFrame(tuple_of_list, columns=['terms', 'mutual information'])\n",
    "    return df_.iloc[:rank+1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_minimum_mi(mutual_information, rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords using Zipf and Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(stopword_list, document):\n",
    "    print('\\nOriginal Doc\\n', document)\n",
    "    \n",
    "    split_space = document['text'].apply(lambda x: x.split())\n",
    "    print('\\nAfter Split by Space\\n', split_space)\n",
    "    \n",
    "    stopword_removal = split_space.apply(lambda x: [ item for item in x if item not in stopword_list ])    \n",
    "    print('\\nAfter Removing Stopwords\\n', stopword_removal)\n",
    "    \n",
    "    join_space = stopword_removal.apply(lambda x: ' '.join(x))\n",
    "    print('\\nAfter Joining by Space\\n', join_space)\n",
    "    \n",
    "    stopword_remove_df = pd.DataFrame(join_space)\n",
    "    return stopword_remove_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_df_zipf = remove_stopwords(stopword_zipf, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_mi = filter_minimum_mi(tuple_of_list=mutual_information, rank=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_df_mi = remove_stopwords(list(stopword_mi['terms']), X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_term(document):\n",
    "    cv = CountVectorizer()\n",
    "    X_st = cv.fit_transform(document['text'])\n",
    "    return cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_term(stopwords_df_zipf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_term(stopwords_df_mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we compute the Cosine Similarity between Query and Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to preprocess the Validation Data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleansing(document=X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_occurences(document=X_valid['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_idf(document=X_valid['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_valid_df = create_dataframe_tuple(list_of_tuple=count_occurences(document=X_valid['text']), columns=['terms', 'freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "occ_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_valid_df = create_dataframe_tuple(list_of_tuple=count_idf(X_valid['text']), columns=['terms', 'idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idf_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_stopword_zipf = filter_freq_zipf(dataframe_tf=occ_valid_df, dataframe_idf=idf_valid_df, threshold_idf=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stopword_zipf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_stopwords_df_zipf = remove_stopwords(valid_stopword_zipf, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank = 7\n",
    "valid_stopword_mi = filter_minimum_mi(tuple_of_list=mutual_info_feature(document=[X_valid, y_valid]), rank=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stopword_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_stopwords_df_mi = remove_stopwords(list(valid_stopword_mi['terms']), X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Results after Removing Stopwords using Mutual Information and Zipf Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stopwords_df_mi = pd.concat([valid_stopwords_df_mi, y_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_stopwords_df_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stopwords_df_zipf = pd.concat([valid_stopwords_df_zipf, y_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_stopwords_df_zipf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords_df_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords_df_mi = pd.concat([stopwords_df_mi, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords_df_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_df_zipf = pd.concat([stopwords_df_zipf, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords_df_zipf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat the Corpus and Query into 1 Dataframe first to Compute the Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using this reference while computing Cosine Similarity:\n",
    "* http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cossim(document_corpus, document_query):\n",
    "    cossim_list = []\n",
    "    for i in range(len(document_query)):\n",
    "        \n",
    "        #Add the query to the Corpus\n",
    "        appended_document = document_corpus.append(document_query.iloc[i])\n",
    "        print('\\nAfter Append Query to Corpus\\n', appended_document)\n",
    "        \n",
    "        cv_cossim = CountVectorizer()\n",
    "        tf_matrix = cv_cossim.fit_transform(appended_document['text'])\n",
    "        \n",
    "        #-1 index means we want to compute cossim between the Query (the last data because appended) and Corpus\n",
    "        cossim = cosine_similarity(tf_matrix[-1], tf_matrix)\n",
    "        \n",
    "        print('Cosine Similarity-', (i+1) ,'th\\n', cossim, '\\n')\n",
    "        print(appended_document.reset_index(drop=True))\n",
    "        #Concat the Appended Document with Cossim Value (Ravel is for flatten the 2d into 1d array)\n",
    "        cossim_df = pd.DataFrame(cossim.ravel(), columns=['cossim'])\n",
    "        concat_cossim = pd.concat([appended_document.reset_index(drop=True), cossim_df], axis=1)\n",
    "        cossim_list.append(concat_cossim)\n",
    "        print('\\nAfter concat with Cossim\\n', concat_cossim)\n",
    "        print('='*100)\n",
    "        \n",
    "    return cossim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_relevant(list_of_cosine_similarity):\n",
    "    cossim_relevant_not = []\n",
    "    for cossim in list_of_cosine_similarity:\n",
    "        query_label = cossim.iloc[-1]['domain']\n",
    "        corpus_label = cossim.iloc[:-1]['domain']\n",
    "        relevant_or_not = [ 'Relevant' if item == query_label else 'Not Relevant' for item in corpus_label ]\n",
    "        ron_df = pd.DataFrame(relevant_or_not, columns=['Relevant/Not Relevant'])\n",
    "        print('\\nRelevant/Not Dataframe\\n', ron_df)\n",
    "        concat_ = pd.concat([cossim, ron_df], axis=1)\n",
    "        concat_ = concat_.sort_values(by='cossim', ascending=False)\n",
    "        print('\\nConcat with Relevant / Not \\n', concat_)\n",
    "        cossim_relevant_not.append(concat_)\n",
    "    return cossim_relevant_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim = compute_cossim(document_corpus=stopwords_df_mi, document_query=valid_stopwords_df_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_not_ = define_relevant(cossim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant_not_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant_not_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_not_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant_not_[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant_not_[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant_not_[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Precision@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(document, k: int):\n",
    "    #we add it with 1 when every document is relevant, and start the loop from 1 till k to avoid the Query which located at first index.\n",
    "    result = [ document.iloc[i]['text'] for i in range(1, k+1) if document.iloc[i]['Relevant/Not Relevant'] == 'Relevant' ]\n",
    "    \n",
    "    #we print the Relevant Documents at K\n",
    "    print('Relevant Documents\\n')\n",
    "    for i, item in enumerate(result):\n",
    "        print(i+1, item)\n",
    "    \n",
    "    return len(result)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_k(relevant_not_[5], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
